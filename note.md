# 概述

## 课程介绍

软件与算法：

- 编程语言
- Caffeine、TensorFlow、Theano等深度学习框架
- OpenCV等计算库

嵌入式硬件实现：

- Verilog实现
- FPGA/ARM 等嵌入式开发环境
- 基本的硬件调试仪器的使用

## 课程内容

### 课程大纲

1. 智能计算加速前沿技术介绍
   1. 智能时代课程教育和人才培养为何必须需要重视“系统能力”
   2. 介绍相关科学技术前沿研究以及产业发展，培养学术研究和技术创新的视野
   3. 为何要针对智能计算任务设计专用的架
2. 智能计算模型与方法
   1. 深度学习、图计算等智能计算的算法模型和原理介绍
   2. 重点介绍深度神经网络计算模型（课程的实践目标在于设计 DNN 加速器）
   3. 介绍 DNN 算法框架、工具、环境等
3. 嵌入式系统加速器设计方法
   1. Zynq FPGA 平台设计开发环境和流程
   2. IP 、总线、接口、 HLS 、时序优化
   3. PS PL 异构加速器设计方法（加速器作为协处理器的设计流程）
4. 算法到硬件的映射与优化方法
   1. 数据流的生成和指令流的映射
   2. 硬件单元的（伪）指令化
   3. 计算单元与数据通路的设计
5. 深度神经网络加速器设计
   1. DNN 加速器的架构设计依据和需求分析
   2. 以 TPU 和 Eyeriss 架构为例进行架构设计
   3. CNN 计算过程的算子化以及数据流生成
6. 加速器的专用集成电路实现方法
   1. CNN 加速器的 FPGA 实现（课程实践）
   2. CNN 加速器的 ASIC 实现需求和区别
   3. ASIC 的设计流程基本介绍
7. 嵌入式深度学习加速器应用前景
   1. DNN 加速器在 CV 等领域的实际应用
   2. 底层架构设计与系统优化之间的关系

### 成绩构成

- 平时作业：30%
- 文献报告：10%
- 课程实验：60%
  - 共6个lab
  - Lab 1 ：神经网络
    - 任务：采用 C/Python 实现一个简单的多层神经网络网络
    - 目的：掌握和理解神经网络的基本原理和计算过程
  - Lab 2 ：深度学习框架
    - 任务：在 Caffe/TensorFlow/ PyTorch 框架上实现 DNN 网络
    - 目的：掌握和理解 DNN 结构和计算流图，并进行简单的调优
  - Lab 3：Zynq FPGA 设计方法
    - 任务：在 Zynq 环境下实现一个矩阵乘加计算模块
    - 目的：掌握开发方法， 包括工具、环境和设计技巧
  - Lab 4：Zynq FPGA 平台的软硬件交互
    - 任务：设计软硬件交互接口，并调用矩阵乘加模块
    - 目的：理解和掌握软硬件交互的逻辑和技巧
  - Lab 5：设计 NaiveTPU 进行矩阵计算加速
    - 任务：在 Zynq 上实现矩阵计算，基于前置实验中的矩阵乘加运算模块以及软硬件交互框架
    - 目的：完成矩阵计算加速的设计与应用，实现基本功能测试
  - Lab 6：设计 NaiveTPU 进行神经网络推理加速
    - 任务：在 Zynq 上实现小型神经网络的计算推理加速
    - 目的：完成神经网络计算加速的实验验证，理解专用硬件设计的加速原理，并进行实验效果的对比分析

## 关于智能计算系统

论文整理：https://github.com/fengbintu/Neural-Networks-on-Silicon

AI芯片整理：https://github.com/basicmi/AI-Chip

学术研究整理：https://github.com/HuaizhengZhang/AI-System-School

# 深度神经网络DNN

## 神经网络基本架构

神经元：接收信息，并给出一个反馈

### 单层感知机

包含两层神经元：

- 输入层：信号传递
- 输出层：MP神经元 threshold logic unit

![单层感知机](https://pigkiller-011955-1319328397.cos.ap-beijing.myqcloud.com/img/202412261623676.png)

## 深度神经网络介绍

神经突触以一定的权重来决定是否激活后续神经元

![DNN](https://pigkiller-011955-1319328397.cos.ap-beijing.myqcloud.com/img/202412261623851.png)

典型的DNN：

- 全连接神经网络
  - 前向传播，多层感知器（MLP）
- 卷积神经网络（CNN）
  - 前向传播，稀疏连接，权值共享
- 循环神经网络（RNN）
  - 反向传播
- Long Short Term Memory：LSTM
  - 反向传播 + 具有记忆性

### DNN的实现阶段

训练阶段（Training）

- 监督学习：有标记过的数据
- 强化学习：通过奖励和惩罚策略来评估
- 无监督学习：没有标记数据
- 弱监督学习：部分数据经过标注

推理阶段（Inference）

- 根据训练好的网络权值来计算输出结果

## 卷积神经网络CNN

CNN：卷积神经网络

卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，可能会多原有层级网络没有的部分

### 层级结构

一个卷积神经网络主要由以下5层组成：

- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层/ ReLU layer
- 池化层 / Pooling layer
- 全连接层/ FC layer

#### 数据输入层

该层要做的处理主要是对原始图像数据进行预处理，其中包括：

- 去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。
- 归一化：幅度归一化到同样的范围，即减少**各维度数据取值范围的差异**而带来的干扰
- PCA/白化：用PCA降维；白化是对数据**各个特征轴上的幅度归一化**

#### 卷积计算层CONV

在这个卷积层，有两个关键操作：

- 局部关联。每个神经元看做一个滤波器（filter）
- 窗口（receptive field）滑动， filter对局部数据计算

在卷积层进行局部感知，提取出图片的特征，然后从更高层次进行局部综合

1. 提取图像的特征，并且**卷积核的权重是可以学习的**，由此可以猜测，在高层神经网络中，卷积操作能突破传统滤波器的限制，根据目标函数提取出想要的特征。
2. “局部感知，参数共享”的特点大大降低了网络参数，保证了网络的稀疏性，防止过拟合，之所以可以“参数共享”，是因为样本存在局部相关的特性。

卷积计算的名词：

- 深度/depth：数据的层数
- 步幅/stride：窗口一次滑动的长度
- 填充值/zero-padding

#### 激活层

所谓激励，实际上是对卷积层的输出结果做一次非线性映射。
$$
f(\sum_{i}w_i x_i + b)
$$
CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱

<img src="https://pigkiller-011955-1319328397.cos.ap-beijing.myqcloud.com/img/202412261623629.png" alt="relu" style="zoom: 67%;" />

如果不用激励函数（其实就相当于激励函数是f(x)=x），这种情况下，每一层的输出都是上一层输入的线性函数。容易得出，**无论有多少神经网络层，输出都是输入的线性组合**，与没有隐层的效果是一样的，这就是最原始的感知机了。

#### 池化层

池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像

在CNN网络中**卷积池之后会跟上一个池化层**，池化层的作用是**提取局部均值与最大值**，根据计算出来的值不一样就分为均值池化层与最大值池化层，**一般常见的多为最大值池化层**。池化的时候同样需要提供filter的大小、步长。

pooling池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。另外一点值得注意：pooling也可以提供一些旋转不变性。

池化层可对提取到的特征信息进行降维，一方面使特征图变小，简化网络计算复杂度并在一定程度上避免过拟合的出现；一方面进行特征压缩，提取主要特征。

池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。

- Max：输出窗口中的最大数
- Ave：输出窗口中的平均数

<img src="https://pigkiller-011955-1319328397.cos.ap-beijing.myqcloud.com/img/202412261623625.jpeg" alt="img" style="zoom:50%;" />

#### 输出层（全连接层）

**经过前面若干次卷积+激励+池化后，终于来到了输出层**，模型会将学到的一个高质量的特征图片全连接层。

其实在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入**dropout操作**，来随机删除神经网络中的部分神经元，**正则化**等来解决此问题。还可以进行**局部归一化（LRN）、数据增强，交叉验证，提前终止训练**等操作，来增加鲁棒性。

全连接层在整个神经网络中起到了分类器的作用：如果说卷积层、池化层和激活函数层等操作是**将原始数据映射到隐层特征空间**的话，全连接层则起到将**学到的“分布式特征表示”映射到样本标记空间**的作用。

### CNN的fine-tuning

fine-tuning就是使用已用于其他目标、预训练好模型的权重或者部分权重，作为初始值开始训练。

那为什么我们不用随机选取选几个数作为权重初始值？原因很简单，第一，自己从头训练卷积神经网络容易出现问题；第二，fine-tuning能很快收敛到一个较理想的状态，省时又省心。

那fine-tuning的具体做法是？

- 复用相同层的权重，**新定义层取随机权重初始值**
- 调大新定义层的的学习率，调小复用层学习率

### CNN总结

卷积网络在本质上是一种**输入到输出的映射**，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。

CNN一个非常重要的特点就是头重脚轻（**越往输入权值越小，越往输出权值越多**），呈现出一个倒三角的形态，这就很好地**避免了BP神经网络中反向传播的时候梯度损失得太快**。

卷积神经网络CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，**避免了显式的特征抽取，而隐式地从训练数据中进行学习**；

再者由于**同一特征映射面上的神经元权值**相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。

卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。

### 相较于BP神经网络

BP全连接，权值太多，需要很多样本去训练，计算困难

卷积神经网络有两种神器可以降低参数数目：

1. 局部感知野
   1. 一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱
   2. 因而，每个神经元其实没有必要对全局图像进行感知，**只需要对局部进行感知**，然后在更高层将局部的信息综合起来就得到了全局的信息。
2. 权值共享
